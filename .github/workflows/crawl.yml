name: Crawl Subsidies
on:
  workflow_dispatch: {}
  schedule: [ { cron: "5 * * * *" } ]

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 12

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with: { python-version: "3.11", cache: "pip" }

      - run: python -m pip install -U pip setuptools wheel
      - run: pip install -r requirements.txt

      - name: Run orchestrator
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          VERTEX_SERVING_CONFIG: ${{ secrets.VERTEX_SERVING_CONFIG }}

          # 月次クォータ（無料枠ガード）
          VERTEX_Q_MONTH_LIMIT: "9000"
          VERTEX_Q_PER_RUN: "50"

          # ウォッチドッグ（10分）
          HARD_KILL_SEC: "600"

          # 速攻寄り（crawl に 4分回す）
          CONNECT_TIMEOUT: "8"
          READ_TIMEOUT: "30"
          CHUSHO_READ_TIMEOUT: "45"
          TIME_BUDGET_SEC: "240"
          MAX_PAGES_PER_RUN: "60"
          MAX_PER_DOMAIN: "20"
          PARALLEL_WORKERS: "6"
          PER_HOST_LIMIT: "2"

          RUN_ID: ${{ github.run_id }}
        run: python orchestrator.py

      - name: Summarize last run (ok & pages)   # ← 完全に置き換え
        continue-on-error: true
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          RUN_ID: ${{ github.run_id }}
        run: |
          python - <<'PY'
          import os, io, csv, psycopg, textwrap
          dsn = os.environ["DATABASE_URL"]
          run = os.environ["RUN_ID"]
          token = f"run={run};"

          ok = ng = skip = s304 = lst = 0
          pages_after = 0
          cand_sum = 0

          with psycopg.connect(dsn, autocommit=True) as c, c.cursor() as cur:
              # pages の現在値（テスト行は除外）…これは固定SQLなので安全
              cur.execute(
                "select count(*) from public.pages "
                "where url not like 'https://example.com/sentinel%'", (), prepare=False
              )
              pages_after = cur.fetchone()[0] or 0

              # fetch_log を CSV で一括取得（SQL内に % を含めない）
              buf = io.StringIO()
              cur.copy(
                "copy (select status, coalesce(error,'') as error from public.fetch_log "
                "where fetched_at > now() - interval '3 hours') to stdout with csv",
                buf
              )
              buf.seek(0)
              reader = csv.reader(buf)
              for status, error in reader:
                  if token not in error:
                      continue
                  if status == 'ok': ok += 1
                  elif status == 'ng': ng += 1
                  elif status == 'skip': skip += 1
                  elif status == '304': s304 += 1
                  elif status == 'list':
                      lst += 1
                      # candidates=NN を抽出
                      i = error.find("candidates=")
                      if i >= 0:
                          j = i + len("candidates=")
                          k = j
                          while k < len(error) and error[k].isdigit():
                              k += 1
                          try:
                              cand_sum += int(error[j:k])
                          except Exception:
                              pass

          md = textwrap.dedent(f"""
          ## Crawl Summary (run {run})
          - Candidates discovered: **{cand_sum}**
          - ok: **{ok}** / 304: {s304} / skip: {skip} / ng: {ng} / list: {lst}
          - pages (non-sentinel) after run: **{pages_after}**
          """)
          open(os.environ['GITHUB_STEP_SUMMARY'], 'a', encoding='utf-8').write(md)
          PY
