name: Crawl Subsidies
on:
  workflow_dispatch: {}
  schedule: [ { cron: "5 * * * *" } ]

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 12

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with: { python-version: "3.11", cache: "pip" }

      - run: python -m pip install -U pip setuptools wheel
      - run: pip install -r requirements.txt

      - name: Run orchestrator
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          VERTEX_SERVING_CONFIG: ${{ secrets.VERTEX_SERVING_CONFIG }}

          # 月次クォータ（無料枠ガード）
          VERTEX_Q_MONTH_LIMIT: "9000"
          VERTEX_Q_PER_RUN: "50"

          # ウォッチドッグ（10分）
          HARD_KILL_SEC: "600"

          # 速攻寄り（crawl に 4分回す）
          CONNECT_TIMEOUT: "8"
          READ_TIMEOUT: "30"
          CHUSHO_READ_TIMEOUT: "45"
          TIME_BUDGET_SEC: "240"
          MAX_PAGES_PER_RUN: "60"
          MAX_PER_DOMAIN: "20"
          PARALLEL_WORKERS: "6"
          PER_HOST_LIMIT: "2"

          RUN_ID: ${{ github.run_id }}
        run: python orchestrator.py
