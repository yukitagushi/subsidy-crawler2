name: Crawl Subsidies
on:
  workflow_dispatch: {}
  schedule: [ { cron: "5 * * * *" } ]

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 12

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with: { python-version: "3.11", cache: "pip" }

      - run: python -m pip install -U pip setuptools wheel
      - run: pip install -r requirements.txt

      - name: Run orchestrator
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          VERTEX_SERVING_CONFIG: ${{ secrets.VERTEX_SERVING_CONFIG }}

          # 月次クォータ（無料枠ガード）
          VERTEX_Q_MONTH_LIMIT: "9000"
          VERTEX_Q_PER_RUN: "50"

          # ウォッチドッグ（10分）
          HARD_KILL_SEC: "600"

          # 速攻寄り(必要に応じて調整)
          CONNECT_TIMEOUT: "8"
          READ_TIMEOUT: "30"
          CHUSHO_READ_TIMEOUT: "45"
          TIME_BUDGET_SEC: "240"
          MAX_PAGES_PER_RUN: "60"
          MAX_PER_DOMAIN: "20"
          PARALLEL_WORKERS: "6"
          PER_HOST_LIMIT: "2"

          RUN_ID: ${{ github.run_id }}
        run: python orchestrator.py

      # 直近RUNの "ok 件数" と "pages の現在値" を安全にサマリ出力（% 不使用）
      - name: Summarize last run (ok & pages)
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          RUN_ID: ${{ github.run_id }}
        run: |
          python - <<'PY'
          import os, psycopg, textwrap
          dsn=os.environ["DATABASE_URL"]
          run=os.environ["RUN_ID"]                 # 数字(文字列)だがSQLには値として渡す

          with psycopg.connect(dsn, autocommit=True) as c, c.cursor() as cur:
              # pages の現在値（テスト行は除外）
              cur.execute(
                "select count(*) from public.pages "
                "where url not like 'https://example.com/sentinel%'", (), prepare=False
              )
              pages_after = cur.fetchone()[0] or 0

              # このRUNの status 別件数
              cur.execute(
                """
                select status, count(*) 
                  from public.fetch_log
                 where position( ('run='||%s||';') in error ) > 0
                 group by status
                """,
                (run,), prepare=False
              )
              counts = {k:v for k,v in cur.fetchall()}

              # このRUNで見つけた候補総数（listログの candidates=XX を合算）
              cur.execute(
                """
                select coalesce(sum((regexp_match(error,'candidates=([0-9]+)'))[1]::int),0)
                  from public.fetch_log
                 where status='list'
                   and position( ('run='||%s||';') in error ) > 0
                """,
                (run,), prepare=False
              )
              cand = cur.fetchone()[0] or 0

          md = textwrap.dedent(f"""
          ## Crawl Summary (run {run})
          - Candidates discovered: **{cand}**
          - ok: **{counts.get('ok',0)}** / 304: {counts.get('304',0)} / skip: {counts.get('skip',0)} / ng: {counts.get('ng',0)} / list: {counts.get('list',0)}
          - pages (non-sentinel) after run: **{pages_after}**
          """)
          open(os.environ['GITHUB_STEP_SUMMARY'],'a',encoding='utf-8').write(md)
          PY
