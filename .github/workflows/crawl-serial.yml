name: Crawl Serial (one-by-one)

on:
  schedule:
    - cron: "*/5 * * * *"     # 5分おき（UTC）。JSTの時間帯だけ走らせるなら 0-9 など範囲指定に変更
  workflow_dispatch: {}

# 直列実行：新しい実行は待機し、今の実行をキャンセルしない
concurrency:
  group: crawl-serial
  cancel-in-progress: false

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 12

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with: { python-version: "3.11", cache: "pip" }

      - run: python -m pip install -U pip setuptools wheel
      - run: pip install -r requirements.txt

      - name: Run orchestrator (serial one-by-one)
        env:
          # === Secrets ===
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          VERTEX_SERVING_CONFIG: ${{ secrets.VERTEX_SERVING_CONFIG }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

          # === シリアルで1件ずつ ===
          SINGLE_BACKFILL_ONE: "1"
          SINGLE_MAX_TRY: "1"               # ← 必ず1件だけ。12分に確実に収まる
          # 3分待ってダメならDR（Stage1）
          SINGLE_STAGE1_READ_TIMEOUT: "180"

          # === DR フォールバック ===
          DR_FETCH_ON_SERIAL: "1"

          # === 接続失敗対策 ===
          SINGLE_FORCE_CONNECT_TIMEOUT: "30"

          # === HEADプリフライト（PDF/巨大を早期判定） ===
          HEAD_CONNECT_TIMEOUT: "8"
          HEAD_READ_TIMEOUT: "6"
          SINGLE_LARGE_BYTES: "8000000"     # 8MB以上はDRへ

          # === 内部ウォッチドッグ：ジョブ上限に余裕を持たせる（9分で強制終了） ===
          HARD_KILL_SEC: "540"

          # 既定（保険）
          CONNECT_TIMEOUT: "8"
          READ_TIMEOUT: "60"

          RUN_ID: ${{ github.run_id }}
        run: python orchestrator.py
