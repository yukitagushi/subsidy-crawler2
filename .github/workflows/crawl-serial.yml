name: Crawl Serial (one-by-one)

on:
  schedule:
    # 5分おき（UTC）。JSTで日中だけにしたい場合は 0-8, 0-14 等に調整してください
    - cron: "*/5 * * * *"
  workflow_dispatch: {}

concurrency:
  group: crawl-serial
  cancel-in-progress: true   # 前の実行が残っていたらキャンセルし最新だけにする

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 12

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with: { python-version: "3.11", cache: "pip" }

      - run: python -m pip install -U pip setuptools wheel
      - run: pip install -r requirements.txt

      - name: Run orchestrator (serial one-by-one)
        env:
          # === Secrets ===
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          VERTEX_SERVING_CONFIG: ${{ secrets.VERTEX_SERVING_CONFIG }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

          # === シリアル処理モード（1件ずつ確実に） ===
          SINGLE_BACKFILL_ONE: "1"        # ← シリアルモードON
          SINGLE_MAX_TRY: "5"             # ← 取れるまで最大5件まで順に試す

          # === じっくり待つ＆DRフォールバック ===
          SINGLE_FORCE_READ_TIMEOUT: "300"   # ← 中小でも READ を 300 秒まで上書き
          DR_FETCH_ON_SERIAL: "1"   
          SINGLE_FORCE_CONNECT_TIMEOUT: "30"   # ← 接続確立まで 30 秒待つ（既定12秒 → 緩和）# ← Deep Research で本文抽出をフォールバック

          # === タイムアウト保険（CONNECTは短め・READは上の強制値を使用） ===
          HARD_KILL_SEC: "600"
          CONNECT_TIMEOUT: "8"
          READ_TIMEOUT: "60"             # 通常READ（シリアル時は SINGLE_FORCE_READ_TIMEOUT が優先）

          RUN_ID: ${{ github.run_id }}
        run: python orchestrator.py
