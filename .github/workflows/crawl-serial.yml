name: Crawl Serial (one-by-one)

on:
  schedule:
    # 5分おき（UTC）。JSTで日中だけにしたい場合は 0-9 のように調整してください
    - cron: "*/5 * * * *"
  workflow_dispatch: {}

# ← 直列キューに変更（新しい実行は待機し、今の実行はキャンセルしない）
concurrency:
  group: crawl-serial
  cancel-in-progress: false

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 12

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with: { python-version: "3.11", cache: "pip" }

      - run: python -m pip install -U pip setuptools wheel
      - run: pip install -r requirements.txt

      - name: Run orchestrator (serial one-by-one)
        env:
          # === Secrets ===
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          VERTEX_SERVING_CONFIG: ${{ secrets.VERTEX_SERVING_CONFIG }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

          # === シリアルで1件ずつ ===
          SINGLE_BACKFILL_ONE: "1"
          SINGLE_MAX_TRY: "2"                  # 12分上限内に収めるため 2 に（必要なら 1 に）
          # 3分待ってダメなら DR に切替（Stage1）
          SINGLE_STAGE1_READ_TIMEOUT: "180"

          # === DR フォールバック有効 ===
          DR_FETCH_ON_SERIAL: "1"

          # === 接続失敗対策 ===
          SINGLE_FORCE_CONNECT_TIMEOUT: "30"

          # === HEAD プリフライト（短時間でPDF/巨大を判定） ===
          HEAD_CONNECT_TIMEOUT: "8"
          HEAD_READ_TIMEOUT: "6"
          SINGLE_LARGE_BYTES: "8000000"        # 8MB 以上は DR 抽出へ

          # === 安全装置 ===
          HARD_KILL_SEC: "600"                 # 10分
          CONNECT_TIMEOUT: "8"
          READ_TIMEOUT: "60"

          RUN_ID: ${{ github.run_id }}
        run: python orchestrator.py
