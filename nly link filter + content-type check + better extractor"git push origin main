[1mdiff --git a/crawl_incremental.py b/crawl_incremental.py[m
[1mindex 6e641bc..0a1c770 100644[m
[1m--- a/crawl_incremental.py[m
[1m+++ b/crawl_incremental.py[m
[36m@@ -1,105 +1,181 @@[m
[31m-import os, time, re, yaml[m
[32m+[m[32m# crawl_incremental.py ‚Äî Â¢óÂàÜ„ÇØ„É≠„Éº„É´ÔºàETag/Last-ModifiedÔºâÔºãÊñáÊõ∏URL/Content-TypeÂà§ÂÆö[m
[32m+[m[32m# ‰æùÂ≠ò: lib/http_client.py, lib/db.py, lib/extractors.py, seeds.yaml[m
[32m+[m[32m# Áí∞Â¢ÉÂ§âÊï∞:[m
[32m+[m[32m#   TIME_BUDGET_SEC (default 240)     ‚Ä¶ „Åì„ÅÆÂÆüË°å„Åß‰Ωø„Åà„ÇãÊúÄÂ§ßÁßíÊï∞[m
[32m+[m[32m#   MAX_PAGES_PER_RUN (default 60)    ‚Ä¶ 1Âõû„ÅÆÂÆüË°å„ÅßË©≥Á¥∞„Éö„Éº„Ç∏„Çí‰øùÂ≠ò„Åô„ÇãÊúÄÂ§ß‰ª∂Êï∞[m
[32m+[m[32m#   MAX_PER_DOMAIN (default 25)       ‚Ä¶ „Éâ„É°„Ç§„É≥„Åî„Å®„ÅÆ‰øùÂ≠ò‰∏äÈôêÔºà1Âõû„ÅÆÂÆüË°åÔºâ[m
[32m+[m[32m#[m
[32m+[m[32m# ‰Ωø„ÅÑÊñπÔºàGitHub ActionsÁ≠âÔºâ:[m
[32m+[m[32m#   python crawl_incremental.py[m
[32m+[m
[32m+[m[32mfrom __future__ import annotations[m
[32m+[m[32mimport os[m
[32m+[m[32mimport re[m
[32m+[m[32mimport time[m
[32m+[m[32mimport yaml[m
[32m+[m[32mfrom typing import Iterable, List, Set[m
 from urllib.parse import urlsplit, urljoin[m
 [m
[31m-from lib.http_client import head_ok, conditional_fetch[m
[32m+[m[32mfrom bs4 import BeautifulSoup[m
[32m+[m
[32m+[m[32mfrom lib.http_client import conditional_fetch, head_ok  # returns (html, etag, last_mod, ctype, status, took_ms)[m
 from lib.extractors import extract_from_html[m
 from lib.db import conn, upsert_http_meta, upsert_page, log_fetch, ensure_schema[m
 [m
[31m-ALLOWED = set()[m
[31m-TIME_BUDGET_SEC = int(os.getenv("TIME_BUDGET_SEC","240"))[m
[31m-MAX_PAGES_PER_RUN = int(os.getenv("MAX_PAGES_PER_RUN","60"))[m
[31m-MAX_PER_DOMAIN = int(os.getenv("MAX_PER_DOMAIN","25"))[m
[32m+[m[32m# -------- Ë®≠ÂÆöÔºàÁí∞Â¢ÉÂ§âÊï∞„Åß‰∏äÊõ∏„ÅçÂèØÔºâ --------[m
[32m+[m[32mTIME_BUDGET_SEC = int(os.getenv("TIME_BUDGET_SEC", "240"))[m
[32m+[m[32mMAX_PAGES_PER_RUN = int(os.getenv("MAX_PAGES_PER_RUN", "60"))[m
[32m+[m[32mMAX_PER_DOMAIN = int(os.getenv("MAX_PER_DOMAIN", "25"))[m
[32m+[m
[32m+[m[32m# Âèó„ÅëÂÖ•„Çå„Çã Content-TypeÔºàÊú¨Êñá„Å®„Åø„Å™„ÅôÔºâ[m
[32m+[m[32mDOC_TYPES: Set[str] = {"text/html", "application/xhtml+xml", "application/pdf"}[m
[32m+[m
[32m+[m[32m# „Ç¢„Çª„ÉÉ„ÉàURL„ÇíÈô§Â§ñÔºàscript/css/ÁîªÂÉè/„Éï„Ç©„É≥„Éà/„Éû„ÉÉ„ÉóÁ≠âÔºâ[m
[32m+[m[32mASSET_RE = re.compile([m
[32m+[m[32m    r'\.(js|mjs|css|png|jpe?g|gif|svg|ico|json|map|woff2?|ttf|eot|mp4|webm)($|\?)',[m
[32m+[m[32m    re.IGNORECASE,[m
[32m+[m[32m)[m
[32m+[m
[32m+[m[32mALLOWED_HOSTS: Set[str] = set()[m
[32m+[m
 [m
 def allowed(u: str) -> bool:[m
[31m-    try: host = urlsplit(u).netloc[m
[31m-    except: return False[m
[31m-    return any(host == d or host.endswith(d) for d in ALLOWED)[m
[32m+[m[32m    """seeds.yaml „ÅÆ allowed_hosts „Å´Âü∫„Å•„ÅçË®±ÂèØ„Éâ„É°„Ç§„É≥„ÅÆ„ÅøÈÄö„Åô"""[m
[32m+[m[32m    try:[m
[32m+[m[32m        host = urlsplit(u).netloc[m
[32m+[m[32m    except Exception:[m
[32m+[m[32m        return False[m
[32m+[m[32m    return any(host == d or host.endswith(d) for d in ALLOWED_HOSTS)[m
[32m+[m
[32m+[m
[32m+[m[32mdef is_document_url(u: str) -> bool:[m
[32m+[m[32m    """Êòé„Çâ„Åã„Å™„Ç¢„Çª„ÉÉ„ÉàURLÔºà.js Á≠âÔºâ„ÇíÈô§Â§ñ"""[m
[32m+[m[32m    if not u.startswith(("http://", "https://")):[m
[32m+[m[32m        return False[m
[32m+[m[32m    if ASSET_RE.search(u):[m
[32m+[m[32m        return False[m
[32m+[m[32m    return True[m
[32m+[m
 [m
[31m-def load_seeds(path="seeds.yaml"):[m
[32m+[m[32mdef load_seeds(path: str = "seeds.yaml") -> List[dict]:[m
     with open(path, "r", encoding="utf-8") as f:[m
         cfg = yaml.safe_load(f)[m
[31m-    global ALLOWED[m
[31m-    ALLOWED = set(cfg.get("allowed_hosts", []))[m
[32m+[m[32m    global ALLOWED_HOSTS[m
[32m+[m[32m    ALLOWED_HOSTS = set(cfg.get("allowed_hosts", []))[m
     return cfg.get("sources", [])[m
 [m
[31m-def find_links(base_url: str, html: str) -> set[str]:[m
[31m-    hrefs = set([m.group(1) for m in re.finditer(r'href=["\']([^"\']+)["\']', html, re.I)])[m
[31m-    out = set()[m
[31m-    for h in hrefs:[m
[31m-        if h.startswith("#") or h.startswith("javascript:"): continue[m
[31m-        u = urljoin(base_url, h)[m
[31m-        if allowed(u): out.add(u)[m
[31m-    return out[m
 [m
[31m-def crawl():[m
[31m-    # ‚òÖ ÊúÄÂàù„Å´„ÉÜ„Éº„Éñ„É´„ÇíÂøÖ„Åö‰Ωú„ÇãÔºàÂàùÂõû„ÇÑÊñ∞DB„Åß„ÇÇËêΩ„Å°„Å™„ÅÑÔºâ[m
[31m-    ensure_schema()[m
[32m+[m[32mdef extract_links(base_url: str, html: str) -> List[str]:[m
[32m+[m[32m    """[m
[32m+[m[32m    <a href="..."> „Å†„Åë„ÇíÂØæË±°„Å´ÊäΩÂá∫„Åó„ÄÅÁµ∂ÂØæURLÂåñ„ÄÇ[m
[32m+[m[32m    script/link„Çø„Ç∞„ÇÑJS„Ç≥„Éº„Éâ‰∏≠„ÅÆURL„ÅØÊãæ„Çè„Å™„ÅÑ„ÄÇ[m
[32m+[m[32m    """[m
[32m+[m[32m    out: List[str] = [][m
[32m+[m[32m    soup = BeautifulSoup(html, "html.parser")[m
[32m+[m[32m    for a in soup.find_all("a", href=True):[m
[32m+[m[32m        href = a.get("href")[m
[32m+[m[32m        if not href or href.startswith("#") or href.startswith("javascript:"):[m
[32m+[m[32m            continue[m
[32m+[m[32m        u = urljoin(base_url, href)[m
[32m+[m[32m        out.append(u)[m
[32m+[m[32m    # ÈáçË§á„ÇíÈô§Â§ñ„ÅóÈ†ÜÂ∫è„ÇíÁ∂≠ÊåÅ[m
[32m+[m[32m    seen = set()[m
[32m+[m[32m    uniq = [][m
[32m+[m[32m    for u in out:[m
[32m+[m[32m        if u not in seen:[m
[32m+[m[32m            seen.add(u)[m
[32m+[m[32m            uniq.append(u)[m
[32m+[m[32m    return uniq[m
[32m+[m
[32m+[m
[32m+[m[32mdef crawl() -> None:[m
[32m+[m[32m    """Â¢óÂàÜ„ÇØ„É≠„Éº„É´Êú¨‰Ωì„ÄÇETag/Last-Modified „Çí‰Ωø„ÅÑ„ÄÅ304 „ÇíÊ¥ªÁî®„ÄÇ"""[m
[32m+[m[32m    ensure_schema()  # ÂàùÂõû„ÇÑÊñ∞DB„Åß„ÇÇËêΩ„Å°„Å™„ÅÑ„Çà„ÅÜ„Å´„Çπ„Ç≠„Éº„ÉûÈÅ©Áî®[m
 [m
     sources = load_seeds()[m
     t_end = time.time() + TIME_BUDGET_SEC[m
     total_saved = 0[m
[31m-    per_domain = {}[m
[32m+[m[32m    saved_per_domain: dict[str, int] = {}[m
 [m
     with conn() as c:[m
         for src in sources:[m
[31m-            if time.time() > t_end: break[m
[31m-            url = src["url"][m
[32m+[m[32m            if time.time() > t_end:[m
[32m+[m[32m                break[m
[32m+[m
[32m+[m[32m            list_url: str = src["url"][m
             include = [re.compile(p) for p in src.get("include", [])][m
             exclude = [re.compile(p) for p in src.get("exclude", [])][m
             max_new = int(src.get("max_new", 20))[m
 [m
[31m-            if not head_ok(url):[m
[31m-                log_fetch(c, url, "skip", 0, "HEAD failed")[m
[32m+[m[32m            # ‰∏ÄË¶ß„Éö„Éº„Ç∏ HEADÔºàÁ∞°ÊòìÊ≠ªÊ¥ªÔºâ[m
[32m+[m[32m            if not head_ok(list_url):[m
[32m+[m[32m                log_fetch(c, list_url, "skip", 0, "HEAD failed")[m
                 continue[m
 [m
[32m+[m[32m            # Êó¢Â≠ò„ÅÆETag/Last-Modified[m
             cur = c.cursor()[m
[31m-            cur.execute("select etag, last_modified from http_cache where url=%s", (url,))[m
[32m+[m[32m            cur.execute("select etag, last_modified from http_cache where url=%s", (list_url,))[m
             row = cur.fetchone()[m
             etag, lm = (row or (None, None))[m
 [m
             try:[m
[31m-                html, new_etag, new_lm, status, took = conditional_fetch(url, etag, lm)[m
[31m-                upsert_http_meta(c, url, new_etag, new_lm, status)[m
[31m-                log_fetch(c, url, "304" if html is None else "ok", took, None)[m
[32m+[m[32m                html, new_etag, new_lm, ctype, status, took = conditional_fetch(list_url, etag, lm)[m
[32m+[m[32m                upsert_http_meta(c, list_url, new_etag, new_lm, status)[m
[32m+[m[32m                log_fetch(c, list_url, "304" if html is None else "ok", took, None)[m
             except Exception as e:[m
[31m-                log_fetch(c, url, "ng", 0, str(e))[m
[32m+[m[32m                log_fetch(c, list_url, "ng", 0, str(e))[m
                 continue[m
 [m
[31m-            if html is None:[m
[32m+[m[32m            # ‰∏ÄË¶ß„Å´Â§âÂåñ„Å™„Åó or Èùû„Éâ„Ç≠„É•„É°„É≥„Éà[m
[32m+[m[32m            if html is None or (ctype and ctype.lower() not in DOC_TYPES):[m
                 continue[m
 [m
[31m-            links = list(find_links(url, html))[m
[31m-            def ok(u):[m
[31m-                if include and not any(p.search(u) for p in include): return False[m
[31m-                if exclude and any(p.search(u) for p in exclude): return False[m
[32m+[m[32m            # ‰∏ÄË¶ß„Åã„ÇâË©≥Á¥∞ÂÄôË£ú„ÇíÊäΩÂá∫‚Üí„Éï„Ç£„É´„Çø[m
[32m+[m[32m            links = extract_links(list_url, html)[m
[32m+[m
[32m+[m[32m            def path_ok(u: str) -> bool:[m
[32m+[m[32m                if include and not any(p.search(u) for p in include):[m
[32m+[m[32m                    return False[m
[32m+[m[32m                if exclude and any(p.search(u) for p in exclude):[m
[32m+[m[32m                    return False[m
                 return True[m
[31m-            cand = [u for u in links if ok(u)][:max_new][m
 [m
[31m-            for u in cand:[m
[32m+[m[32m            candidates = [u for u in links if allowed(u) and is_document_url(u) and path_ok(u)][:max_new][m
[32m+[m
[32m+[m[32m            for u in candidates:[m
                 if time.time() > t_end or total_saved >= MAX_PAGES_PER_RUN:[m
                     break[m
                 host = urlsplit(u).netloc[m
[31m-                per_domain[host] = per_domain.get(host, 0) + 1[m
[31m-                if per_domain[host] > MAX_PER_DOMAIN:[m
[32m+[m[32m                saved_per_domain[host] = saved_per_domain.get(host, 0) + 1[m
[32m+[m[32m                if saved_per_domain[host] > MAX_PER_DOMAIN:[m
                     continue[m
 [m
[32m+[m[32m                # Êó¢Â≠ò„ÅÆETag/Last-Modified[m
                 cur.execute("select etag, last_modified from http_cache where url=%s", (u,))[m
                 row = cur.fetchone()[m
                 petag, plm = (row or (None, None))[m
 [m
                 try:[m
[31m-                    html, new_etag, new_lm, status, took = conditional_fetch(u, petag, plm)[m
[32m+[m[32m                    html, new_etag, new_lm, ctype, status, took = conditional_fetch(u, petag, plm)[m
                     upsert_http_meta(c, u, new_etag, new_lm, status)[m
[32m+[m
[32m+[m[32m                    # 304 or ÈùûÊñáÊõ∏„ÅØÈô§Â§ñ[m
                     if html is None:[m
                         log_fetch(c, u, "304", took, None)[m
                         continue[m
[31m-                    row = extract_from_html(u, html)[m
[31m-                    changed = upsert_page(c, row)[m
[32m+[m[32m                    if ctype and ctype.lower() not in DOC_TYPES:[m
[32m+[m[32m                        log_fetch(c, u, "skip", took, f"ctype={ctype}")[m
[32m+[m[32m                        continue[m
[32m+[m
[32m+[m[32m                    row_dict = extract_from_html(u, html)[m
[32m+[m[32m                    changed = upsert_page(c, row_dict)[m
                     log_fetch(c, u, "ok" if changed else "skip", took, None)[m
[31m-                    if changed: total_saved += 1[m
[32m+[m[32m                    if changed:[m
[32m+[m[32m                        total_saved += 1[m
                 except Exception as e:[m
                     log_fetch(c, u, "ng", 0, str(e))[m
                     continue[m
 [m
[32m+[m
 if __name__ == "__main__":[m
     crawl()[m
\ No newline at end of file[m
[1mdiff --git a/lib/extractors.py b/lib/extractors.py[m
[1mindex c62cae9..0dd1372 100644[m
[1m--- a/lib/extractors.py[m
[1m+++ b/lib/extractors.py[m
[36m@@ -1,40 +1,85 @@[m
[32m+[m[32m# lib/extractors.py ‚Äî HTML „Åã„ÇâË¶ÅÁÇπÊäΩÂá∫Ôºà„Çø„Ç§„Éà„É´„ÉªË¶ÅÁ¥Ñ„ÉªÂπ¥Â∫¶„ÉªË£úÂä©Áéá„Å™„Å©Ôºâ[m
[32m+[m[32mfrom __future__ import annotations[m
 import re[m
 from bs4 import BeautifulSoup[m
 from .util import norm_ws, clip[m
 [m
[32m+[m[32mdef _meta(soup: BeautifulSoup, *pairs: tuple[str, str]) -> str:[m
[32m+[m[32m    """[m
[32m+[m[32m    ‰æã: _meta(soup, ("property","og:title"), ("name","twitter:title"))[m
[32m+[m[32m    ‰∏ä„Åã„ÇâÈ†Ü„Å´Êé¢„Åó„ÄÅcontent „ÇíËøî„Åô[m
[32m+[m[32m    """[m
[32m+[m[32m    for k, v in pairs:[m
[32m+[m[32m        m = soup.find("meta", attrs={k: v})[m
[32m+[m[32m        if m and m.get("content"):[m
[32m+[m[32m            return norm_ws(m["content"])[m
[32m+[m[32m    return ""[m
[32m+[m
[32m+[m
 def extract_from_html(url: str, html: str) -> dict:[m
[32m+[m[32m    """[m
[32m+[m[32m    „Çø„Ç§„Éà„É´„ÅåÂèñ„Çå„Å™„ÅÑSPAÁ≠â„Å´„ÇÇÂØæÂøú:[m
[32m+[m[32m      - <title> ‚Üí og:title ‚Üí twitter:title ‚Üí summaryÂÖàÈ†≠ „ÅßË£úÂÆå[m
[32m+[m[32m      - Âπ¥Â∫¶: ÂíåÊö¶(‰ª§ÂíåxÂπ¥Â∫¶) / Ë•øÊö¶(2025Âπ¥Â∫¶) „Çí‰∏°ÂØæÂøú[m
[32m+[m[32m      - ‰ª£Ë°®ÁöÑ„Å™„Éï„Ç£„Éº„É´„Éâ„ÇíÁ∑©„ÅÑÊ≠£Ë¶èË°®Áèæ„ÅßÊäΩÂá∫[m
[32m+[m[32m    """[m
     soup = BeautifulSoup(html, "html.parser")[m
[31m-    title = norm_ws(soup.title.text if soup.title else "")[m
[31m-    desc  = soup.find("meta", attrs={"name":"description"})[m
[31m-    summary = norm_ws(desc.get("content","")) if desc else ""[m
[31m-    if not summary:[m
[31m-        p = soup.find("p")[m
[31m-        summary = norm_ws(p.get_text(" ")) if p else ""[m
[31m-    h = soup.find(["h1","h2","h3"])[m
[31m-    if h and not title: title = norm_ws(h.get_text(" "))[m
 [m
[32m+[m[32m    # „Çø„Ç§„Éà„É´[m
[32m+[m[32m    title = norm_ws(soup.title.text if soup.title else "") \[m
[32m+[m[32m        or _meta(soup, ("property", "og:title"), ("name", "twitter:title"))[m
[32m+[m
[32m+[m[32m    # Ë¶ÅÁ¥ÑÔºàmeta description ‚Üí ÂÖàÈ†≠pÔºâ[m
[32m+[m[32m    desc = _meta(soup, ("name", "description"), ("property", "og:description"))[m
[32m+[m[32m    if desc:[m
[32m+[m[32m        summary = desc[m
[32m+[m[32m    else:[m
[32m+[m[32m        first_p = soup.find("p")[m
[32m+[m[32m        summary = norm_ws(first_p.get_text(" ")) if first_p else ""[m
[32m+[m
[32m+[m[32m    if not title:[m
[32m+[m[32m        # „Çø„Ç§„Éà„É´„ÅåÁÑ°„ÅÑ„Å®„Åç„ÅØ summary ÂÖàÈ†≠„ÅßË£úÂÆå[m
[32m+[m[32m        title = (summary[:40] or "(ÁÑ°È°å)")[m
[32m+[m
[32m+[m[32m    # „Éö„Éº„Ç∏„ÉÜ„Ç≠„Çπ„Éà[m
     text = soup.get_text(" ")[m
 [m
[31m-    def f(p): [m
[31m-        m = re.search(p, text)[m
[31m-        return norm_ws(m.group(1 if m and m.lastindex else 0)) if m else None[m
[32m+[m[32m    def f(pat: str) -> str | None:[m
[32m+[m[32m        m = re.search(pat, text)[m
[32m+[m[32m        if not m:[m
[32m+[m[32m            return None[m
[32m+[m[32m        g = m.group(1 if m.lastindex else 0)[m
[32m+[m[32m        return norm_ws(g)[m
 [m
[31m-    rate = f(r"Ë£úÂä©Áéá[\s:Ôºö]*([0-9Ôºê-Ôºô]+ ?%?)")[m
[31m-    cap  = f(r"‰∏äÈôê[\s:Ôºö]*([0-9Ôºê-Ôºô,Ôºå]+ ?(?:ÂÜÜ|‰∏áÂÜÜ|ÂÑÑÂÜÜ)?)")[m
[31m-    fy   = f(r"(‰ª§Âíå\s*[0-9Ôºê-Ôºô]+Âπ¥Â∫¶)")[m
[31m-    cn   = f(r"Á¨¨\s*([0-9Ôºê-Ôºô]+)Âõû")[m
[32m+[m[32m    # Âπ¥Â∫¶ÔºàÂíåÊö¶ or Ë•øÊö¶Ôºâ[m
[32m+[m[32m    fiscal_year = f(r"(‰ª§Âíå\s*[0-9Ôºê-Ôºô]+Âπ¥Â∫¶|20[0-9]{2}Âπ¥Â∫¶)")[m
[32m+[m[32m    call_no     = f(r"Á¨¨\s*([0-9Ôºê-Ôºô]+)\s*Âõû")[m
[32m+[m[32m    rate        = f(r"Ë£úÂä©Áéá[\s:Ôºö]*([0-9Ôºê-Ôºô]+ ?%?)")[m
[32m+[m[32m    cap         = f(r"‰∏äÈôê[\s:Ôºö]*([0-9Ôºê-Ôºô,Ôºå]+ ?(?:ÂÜÜ|‰∏áÂÜÜ|ÂÑÑÂÜÜ)?)")[m
 [m
[32m+[m[32m    # ÂØæË±°/ÂØæË±°ÁµåË≤ªÔºàË¶ãÂá∫„ÅóÁõ¥Âæå„ÅÆË°å„Çí„É©„Éï„Å´Êäú„ÅèÔºâ[m
     target, cost_items = None, None[m
[31m-    for lab in ("ÂØæË±°ÁµåË≤ª","ÂØæË±°ËÄÖ","ÂØæË±°"):[m
[32m+[m[32m    for lab in ("ÂØæË±°ÁµåË≤ª", "ÂØæË±°ËÄÖ", "ÂØæË±°"):[m
         m = re.search(lab + r"[\s:Ôºö]*(.+?)\n", text)[m
         if m:[m
             val = norm_ws(m.group(1))[m
[31m-            if "ÁµåË≤ª" in lab: cost_items = val[m
[31m-            else: target = val[m
[31m-[m
[31m-    return dict([m
[31m-      url=url, title=title or "(ÁÑ°È°å)", summary=clip(summary, 800),[m
[31m-      rate=rate, cap=cap, target=target, cost_items=cost_items,[m
[31m-      deadline=None, fiscal_year=fy, call_no=cn, scheme_type=None,[m
[31m-      period_from=None, period_to=None[m
[31m-    )[m
\ No newline at end of file[m
[32m+[m[32m            if "ÁµåË≤ª" in lab:[m
[32m+[m[32m                cost_items = val[m
[32m+[m[32m            else:[m
[32m+[m[32m                target = val[m
[32m+[m
[32m+[m[32m    return {[m
[32m+[m[32m        "url": url,[m
[32m+[m[32m        "title": title or "(ÁÑ°È°å)",[m
[32m+[m[32m        "summary": clip(summary, 800),[m
[32m+[m[32m        "rate": rate,[m
[32m+[m[32m        "cap": cap,[m
[32m+[m[32m        "target": target,[m
[32m+[m[32m        "cost_items": cost_items,[m
[32m+[m[32m        "deadline": None,[m
[32m+[m[32m        "fiscal_year": fiscal_year,[m
[32m+[m[32m        "call_no": call_no,[m
[32m+[m[32m        "scheme_type": None,[m
[32m+[m[32m        "period_from": None,[m
[32m+[m[32m        "period_to": None,[m
[32m+[m[32m    }[m
\ No newline at end of file[m
[1mdiff --git a/lib/http_client.py b/lib/http_client.py[m
[1mindex a15e79e..8ec20bd 100644[m
[1m--- a/lib/http_client.py[m
[1m+++ b/lib/http_client.py[m
[36m@@ -21,15 +21,15 @@[m [mdef head_ok(u: str, ct=3, rt=5) -> bool:[m
         return False[m
 [m
 def conditional_fetch(u: str, etag: str | None, last_mod: str | None):[m
[31m-    """If-None-Match / If-Modified-Since „Çí‰ªò„Åë„Å¶ÂèñÂæó„ÄÇ304„Å™„Çâ (None, etag, last_mod) „ÇíËøî„Åô„ÄÇ"""[m
     hdr = dict(HEADERS)[m
     if etag: hdr["If-None-Match"] = etag[m
     if last_mod: hdr["If-Modified-Since"] = last_mod[m
     t0 = time.time()[m
     r = S.get(u, headers=hdr, timeout=(CONNECT, READ))[m
     took_ms = int((time.time() - t0) * 1000)[m
[31m-    r.raise_for_status() if r.status_code != 304 else None[m
[31m-    new_etag = r.headers.get("ETag") or etag[m
[31m-    new_lm   = r.headers.get("Last-Modified") or last_mod[m
[31m-    return (None if r.status_code == 304 else r.text, new_etag, new_lm, r.status_code, took_ms)[m
[32m+[m[32m    ctype = (r.headers.get("Content-Type") or "").split(";")[0].lower()[m
[32m+[m[32m    if r.status_code == 304:[m
[32m+[m[32m        return None, etag, last_mod, ctype, r.status_code, took_ms[m
[32m+[m[32m    r.raise_for_status()[m
[32m+[m[32m    return r.text, r.headers.get("ETag") or etag, r.headers.get("Last-Modified") or last_mod, ctype, r.status_code, took_ms[m
     [m
\ No newline at end of file[m
