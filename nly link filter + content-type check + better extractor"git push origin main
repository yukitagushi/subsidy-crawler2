[1mdiff --git a/crawl_incremental.py b/crawl_incremental.py[m
[1mindex 6e641bc..0a1c770 100644[m
[1m--- a/crawl_incremental.py[m
[1m+++ b/crawl_incremental.py[m
[36m@@ -1,105 +1,181 @@[m
[31m-import os, time, re, yaml[m
[32m+[m[32m# crawl_incremental.py â€” å¢—åˆ†ã‚¯ãƒ­ãƒ¼ãƒ«ï¼ˆETag/Last-Modifiedï¼‰ï¼‹æ–‡æ›¸URL/Content-Typeåˆ¤å®š[m
[32m+[m[32m# ä¾å­˜: lib/http_client.py, lib/db.py, lib/extractors.py, seeds.yaml[m
[32m+[m[32m# ç’°å¢ƒå¤‰æ•°:[m
[32m+[m[32m#   TIME_BUDGET_SEC (default 240)     â€¦ ã“ã®å®Ÿè¡Œã§ä½¿ãˆã‚‹æœ€å¤§ç§’æ•°[m
[32m+[m[32m#   MAX_PAGES_PER_RUN (default 60)    â€¦ 1å›ã®å®Ÿè¡Œã§è©³ç´°ãƒšãƒ¼ã‚¸ã‚’ä¿å­˜ã™ã‚‹æœ€å¤§ä»¶æ•°[m
[32m+[m[32m#   MAX_PER_DOMAIN (default 25)       â€¦ ãƒ‰ãƒ¡ã‚¤ãƒ³ã”ã¨ã®ä¿å­˜ä¸Šé™ï¼ˆ1å›ã®å®Ÿè¡Œï¼‰[m
[32m+[m[32m#[m
[32m+[m[32m# ä½¿ã„æ–¹ï¼ˆGitHub Actionsç­‰ï¼‰:[m
[32m+[m[32m#   python crawl_incremental.py[m
[32m+[m
[32m+[m[32mfrom __future__ import annotations[m
[32m+[m[32mimport os[m
[32m+[m[32mimport re[m
[32m+[m[32mimport time[m
[32m+[m[32mimport yaml[m
[32m+[m[32mfrom typing import Iterable, List, Set[m
 from urllib.parse import urlsplit, urljoin[m
 [m
[31m-from lib.http_client import head_ok, conditional_fetch[m
[32m+[m[32mfrom bs4 import BeautifulSoup[m
[32m+[m
[32m+[m[32mfrom lib.http_client import conditional_fetch, head_ok  # returns (html, etag, last_mod, ctype, status, took_ms)[m
 from lib.extractors import extract_from_html[m
 from lib.db import conn, upsert_http_meta, upsert_page, log_fetch, ensure_schema[m
 [m
[31m-ALLOWED = set()[m
[31m-TIME_BUDGET_SEC = int(os.getenv("TIME_BUDGET_SEC","240"))[m
[31m-MAX_PAGES_PER_RUN = int(os.getenv("MAX_PAGES_PER_RUN","60"))[m
[31m-MAX_PER_DOMAIN = int(os.getenv("MAX_PER_DOMAIN","25"))[m
[32m+[m[32m# -------- è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãå¯ï¼‰ --------[m
[32m+[m[32mTIME_BUDGET_SEC = int(os.getenv("TIME_BUDGET_SEC", "240"))[m
[32m+[m[32mMAX_PAGES_PER_RUN = int(os.getenv("MAX_PAGES_PER_RUN", "60"))[m
[32m+[m[32mMAX_PER_DOMAIN = int(os.getenv("MAX_PER_DOMAIN", "25"))[m
[32m+[m
[32m+[m[32m# å—ã‘å…¥ã‚Œã‚‹ Content-Typeï¼ˆæœ¬æ–‡ã¨ã¿ãªã™ï¼‰[m
[32m+[m[32mDOC_TYPES: Set[str] = {"text/html", "application/xhtml+xml", "application/pdf"}[m
[32m+[m
[32m+[m[32m# ã‚¢ã‚»ãƒƒãƒˆURLã‚’é™¤å¤–ï¼ˆscript/css/ç”»åƒ/ãƒ•ã‚©ãƒ³ãƒˆ/ãƒãƒƒãƒ—ç­‰ï¼‰[m
[32m+[m[32mASSET_RE = re.compile([m
[32m+[m[32m    r'\.(js|mjs|css|png|jpe?g|gif|svg|ico|json|map|woff2?|ttf|eot|mp4|webm)($|\?)',[m
[32m+[m[32m    re.IGNORECASE,[m
[32m+[m[32m)[m
[32m+[m
[32m+[m[32mALLOWED_HOSTS: Set[str] = set()[m
[32m+[m
 [m
 def allowed(u: str) -> bool:[m
[31m-    try: host = urlsplit(u).netloc[m
[31m-    except: return False[m
[31m-    return any(host == d or host.endswith(d) for d in ALLOWED)[m
[32m+[m[32m    """seeds.yaml ã® allowed_hosts ã«åŸºã¥ãè¨±å¯ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ã¿é€šã™"""[m
[32m+[m[32m    try:[m
[32m+[m[32m        host = urlsplit(u).netloc[m
[32m+[m[32m    except Exception:[m
[32m+[m[32m        return False[m
[32m+[m[32m    return any(host == d or host.endswith(d) for d in ALLOWED_HOSTS)[m
[32m+[m
[32m+[m
[32m+[m[32mdef is_document_url(u: str) -> bool:[m
[32m+[m[32m    """æ˜ã‚‰ã‹ãªã‚¢ã‚»ãƒƒãƒˆURLï¼ˆ.js ç­‰ï¼‰ã‚’é™¤å¤–"""[m
[32m+[m[32m    if not u.startswith(("http://", "https://")):[m
[32m+[m[32m        return False[m
[32m+[m[32m    if ASSET_RE.search(u):[m
[32m+[m[32m        return False[m
[32m+[m[32m    return True[m
[32m+[m
 [m
[31m-def load_seeds(path="seeds.yaml"):[m
[32m+[m[32mdef load_seeds(path: str = "seeds.yaml") -> List[dict]:[m
     with open(path, "r", encoding="utf-8") as f:[m
         cfg = yaml.safe_load(f)[m
[31m-    global ALLOWED[m
[31m-    ALLOWED = set(cfg.get("allowed_hosts", []))[m
[32m+[m[32m    global ALLOWED_HOSTS[m
[32m+[m[32m    ALLOWED_HOSTS = set(cfg.get("allowed_hosts", []))[m
     return cfg.get("sources", [])[m
 [m
[31m-def find_links(base_url: str, html: str) -> set[str]:[m
[31m-    hrefs = set([m.group(1) for m in re.finditer(r'href=["\']([^"\']+)["\']', html, re.I)])[m
[31m-    out = set()[m
[31m-    for h in hrefs:[m
[31m-        if h.startswith("#") or h.startswith("javascript:"): continue[m
[31m-        u = urljoin(base_url, h)[m
[31m-        if allowed(u): out.add(u)[m
[31m-    return out[m
 [m
[31m-def crawl():[m
[31m-    # â˜… æœ€åˆã«ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å¿…ãšä½œã‚‹ï¼ˆåˆå›ã‚„æ–°DBã§ã‚‚è½ã¡ãªã„ï¼‰[m
[31m-    ensure_schema()[m
[32m+[m[32mdef extract_links(base_url: str, html: str) -> List[str]:[m
[32m+[m[32m    """[m
[32m+[m[32m    <a href="..."> ã ã‘ã‚’å¯¾è±¡ã«æŠ½å‡ºã—ã€çµ¶å¯¾URLåŒ–ã€‚[m
[32m+[m[32m    script/linkã‚¿ã‚°ã‚„JSã‚³ãƒ¼ãƒ‰ä¸­ã®URLã¯æ‹¾ã‚ãªã„ã€‚[m
[32m+[m[32m    """[m
[32m+[m[32m    out: List[str] = [][m
[32m+[m[32m    soup = BeautifulSoup(html, "html.parser")[m
[32m+[m[32m    for a in soup.find_all("a", href=True):[m
[32m+[m[32m        href = a.get("href")[m
[32m+[m[32m        if not href or href.startswith("#") or href.startswith("javascript:"):[m
[32m+[m[32m            continue[m
[32m+[m[32m        u = urljoin(base_url, href)[m
[32m+[m[32m        out.append(u)[m
[32m+[m[32m    # é‡è¤‡ã‚’é™¤å¤–ã—é †åºã‚’ç¶­æŒ[m
[32m+[m[32m    seen = set()[m
[32m+[m[32m    uniq = [][m
[32m+[m[32m    for u in out:[m
[32m+[m[32m        if u not in seen:[m
[32m+[m[32m            seen.add(u)[m
[32m+[m[32m            uniq.append(u)[m
[32m+[m[32m    return uniq[m
[32m+[m
[32m+[m
[32m+[m[32mdef crawl() -> None:[m
[32m+[m[32m    """å¢—åˆ†ã‚¯ãƒ­ãƒ¼ãƒ«æœ¬ä½“ã€‚ETag/Last-Modified ã‚’ä½¿ã„ã€304 ã‚’æ´»ç”¨ã€‚"""[m
[32m+[m[32m    ensure_schema()  # åˆå›ã‚„æ–°DBã§ã‚‚è½ã¡ãªã„ã‚ˆã†ã«ã‚¹ã‚­ãƒ¼ãƒé©ç”¨[m
 [m
     sources = load_seeds()[m
     t_end = time.time() + TIME_BUDGET_SEC[m
     total_saved = 0[m
[31m-    per_domain = {}[m
[32m+[m[32m    saved_per_domain: dict[str, int] = {}[m
 [m
     with conn() as c:[m
         for src in sources:[m
[31m-            if time.time() > t_end: break[m
[31m-            url = src["url"][m
[32m+[m[32m            if time.time() > t_end:[m
[32m+[m[32m                break[m
[32m+[m
[32m+[m[32m            list_url: str = src["url"][m
             include = [re.compile(p) for p in src.get("include", [])][m
             exclude = [re.compile(p) for p in src.get("exclude", [])][m
             max_new = int(src.get("max_new", 20))[m
 [m
[31m-            if not head_ok(url):[m
[31m-                log_fetch(c, url, "skip", 0, "HEAD failed")[m
[32m+[m[32m            # ä¸€è¦§ãƒšãƒ¼ã‚¸ HEADï¼ˆç°¡æ˜“æ­»æ´»ï¼‰[m
[32m+[m[32m            if not head_ok(list_url):[m
[32m+[m[32m                log_fetch(c, list_url, "skip", 0, "HEAD failed")[m
                 continue[m
 [m
[32m+[m[32m            # æ—¢å­˜ã®ETag/Last-Modified[m
             cur = c.cursor()[m
[31m-            cur.execute("select etag, last_modified from http_cache where url=%s", (url,))[m
[32m+[m[32m            cur.execute("select etag, last_modified from http_cache where url=%s", (list_url,))[m
             row = cur.fetchone()[m
             etag, lm = (row or (None, None))[m
 [m
             try:[m
[31m-                html, new_etag, new_lm, status, took = conditional_fetch(url, etag, lm)[m
[31m-                upsert_http_meta(c, url, new_etag, new_lm, status)[m
[31m-                log_fetch(c, url, "304" if html is None else "ok", took, None)[m
[32m+[m[32m                html, new_etag, new_lm, ctype, status, took = conditional_fetch(list_url, etag, lm)[m
[32m+[m[32m                upsert_http_meta(c, list_url, new_etag, new_lm, status)[m
[32m+[m[32m                log_fetch(c, list_url, "304" if html is None else "ok", took, None)[m
             except Exception as e:[m
[31m-                log_fetch(c, url, "ng", 0, str(e))[m
[32m+[m[32m                log_fetch(c, list_url, "ng", 0, str(e))[m
                 continue[m
 [m
[31m-            if html is None:[m
[32m+[m[32m            # ä¸€è¦§ã«å¤‰åŒ–ãªã— or éãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ[m
[32m+[m[32m            if html is None or (ctype and ctype.lower() not in DOC_TYPES):[m
                 continue[m
 [m
[31m-            links = list(find_links(url, html))[m
[31m-            def ok(u):[m
[31m-                if include and not any(p.search(u) for p in include): return False[m
[31m-                if exclude and any(p.search(u) for p in exclude): return False[m
[32m+[m[32m            # ä¸€è¦§ã‹ã‚‰è©³ç´°å€™è£œã‚’æŠ½å‡ºâ†’ãƒ•ã‚£ãƒ«ã‚¿[m
[32m+[m[32m            links = extract_links(list_url, html)[m
[32m+[m
[32m+[m[32m            def path_ok(u: str) -> bool:[m
[32m+[m[32m                if include and not any(p.search(u) for p in include):[m
[32m+[m[32m                    return False[m
[32m+[m[32m                if exclude and any(p.search(u) for p in exclude):[m
[32m+[m[32m                    return False[m
                 return True[m
[31m-            cand = [u for u in links if ok(u)][:max_new][m
 [m
[31m-            for u in cand:[m
[32m+[m[32m            candidates = [u for u in links if allowed(u) and is_document_url(u) and path_ok(u)][:max_new][m
[32m+[m
[32m+[m[32m            for u in candidates:[m
                 if time.time() > t_end or total_saved >= MAX_PAGES_PER_RUN:[m
                     break[m
                 host = urlsplit(u).netloc[m
[31m-                per_domain[host] = per_domain.get(host, 0) + 1[m
[31m-                if per_domain[host] > MAX_PER_DOMAIN:[m
[32m+[m[32m                saved_per_domain[host] = saved_per_domain.get(host, 0) + 1[m
[32m+[m[32m                if saved_per_domain[host] > MAX_PER_DOMAIN:[m
                     continue[m
 [m
[32m+[m[32m                # æ—¢å­˜ã®ETag/Last-Modified[m
                 cur.execute("select etag, last_modified from http_cache where url=%s", (u,))[m
                 row = cur.fetchone()[m
                 petag, plm = (row or (None, None))[m
 [m
                 try:[m
[31m-                    html, new_etag, new_lm, status, took = conditional_fetch(u, petag, plm)[m
[32m+[m[32m                    html, new_etag, new_lm, ctype, status, took = conditional_fetch(u, petag, plm)[m
                     upsert_http_meta(c, u, new_etag, new_lm, status)[m
[32m+[m
[32m+[m[32m                    # 304 or éæ–‡æ›¸ã¯é™¤å¤–[m
                     if html is None:[m
                         log_fetch(c, u, "304", took, None)[m
                         continue[m
[31m-                    row = extract_from_html(u, html)[m
[31m-                    changed = upsert_page(c, row)[m
[32m+[m[32m                    if ctype and ctype.lower() not in DOC_TYPES:[m
[32m+[m[32m                        log_fetch(c, u, "skip", took, f"ctype={ctype}")[m
[32m+[m[32m                        continue[m
[32m+[m
[32m+[m[32m                    row_dict = extract_from_html(u, html)[m
[32m+[m[32m                    changed = upsert_page(c, row_dict)[m
                     log_fetch(c, u, "ok" if changed else "skip", took, None)[m
[31m-                    if changed: total_saved += 1[m
[32m+[m[32m                    if changed:[m
[32m+[m[32m                        total_saved += 1[m
                 except Exception as e:[m
                     log_fetch(c, u, "ng", 0, str(e))[m
                     continue[m
 [m
[32m+[m
 if __name__ == "__main__":[m
     crawl()[m
\ No newline at end of file[m
[1mdiff --git a/lib/extractors.py b/lib/extractors.py[m
[1mindex c62cae9..0dd1372 100644[m
[1m--- a/lib/extractors.py[m
[1m+++ b/lib/extractors.py[m
[36m@@ -1,40 +1,85 @@[m
[32m+[m[32m# lib/extractors.py â€” HTML ã‹ã‚‰è¦ç‚¹æŠ½å‡ºï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãƒ»è¦ç´„ãƒ»å¹´åº¦ãƒ»è£œåŠ©ç‡ãªã©ï¼‰[m
[32m+[m[32mfrom __future__ import annotations[m
 import re[m
 from bs4 import BeautifulSoup[m
 from .util import norm_ws, clip[m
 [m
[32m+[m[32mdef _meta(soup: BeautifulSoup, *pairs: tuple[str, str]) -> str:[m
[32m+[m[32m    """[m
[32m+[m[32m    ä¾‹: _meta(soup, ("property","og:title"), ("name","twitter:title"))[m
[32m+[m[32m    ä¸Šã‹ã‚‰é †ã«æ¢ã—ã€content ã‚’è¿”ã™[m
[32m+[m[32m    """[m
[32m+[m[32m    for k, v in pairs:[m
[32m+[m[32m        m = soup.find("meta", attrs={k: v})[m
[32m+[m[32m        if m and m.get("content"):[m
[32m+[m[32m            return norm_ws(m["content"])[m
[32m+[m[32m    return ""[m
[32m+[m
[32m+[m
 def extract_from_html(url: str, html: str) -> dict:[m
[32m+[m[32m    """[m
[32m+[m[32m    ã‚¿ã‚¤ãƒˆãƒ«ãŒå–ã‚Œãªã„SPAç­‰ã«ã‚‚å¯¾å¿œ:[m
[32m+[m[32m      - <title> â†’ og:title â†’ twitter:title â†’ summaryå…ˆé ­ ã§è£œå®Œ[m
[32m+[m[32m      - å¹´åº¦: å’Œæš¦(ä»¤å’Œxå¹´åº¦) / è¥¿æš¦(2025å¹´åº¦) ã‚’ä¸¡å¯¾å¿œ[m
[32m+[m[32m      - ä»£è¡¨çš„ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ç·©ã„æ­£è¦è¡¨ç¾ã§æŠ½å‡º[m
[32m+[m[32m    """[m
     soup = BeautifulSoup(html, "html.parser")[m
[31m-    title = norm_ws(soup.title.text if soup.title else "")[m
[31m-    desc  = soup.find("meta", attrs={"name":"description"})[m
[31m-    summary = norm_ws(desc.get("content","")) if desc else ""[m
[31m-    if not summary:[m
[31m-        p = soup.find("p")[m
[31m-        summary = norm_ws(p.get_text(" ")) if p else ""[m
[31m-    h = soup.find(["h1","h2","h3"])[m
[31m-    if h and not title: title = norm_ws(h.get_text(" "))[m
 [m
[32m+[m[32m    # ã‚¿ã‚¤ãƒˆãƒ«[m
[32m+[m[32m    title = norm_ws(soup.title.text if soup.title else "") \[m
[32m+[m[32m        or _meta(soup, ("property", "og:title"), ("name", "twitter:title"))[m
[32m+[m
[32m+[m[32m    # è¦ç´„ï¼ˆmeta description â†’ å…ˆé ­pï¼‰[m
[32m+[m[32m    desc = _meta(soup, ("name", "description"), ("property", "og:description"))[m
[32m+[m[32m    if desc:[m
[32m+[m[32m        summary = desc[m
[32m+[m[32m    else:[m
[32m+[m[32m        first_p = soup.find("p")[m
[32m+[m[32m        summary = norm_ws(first_p.get_text(" ")) if first_p else ""[m
[32m+[m
[32m+[m[32m    if not title:[m
[32m+[m[32m        # ã‚¿ã‚¤ãƒˆãƒ«ãŒç„¡ã„ã¨ãã¯ summary å…ˆé ­ã§è£œå®Œ[m
[32m+[m[32m        title = (summary[:40] or "(ç„¡é¡Œ)")[m
[32m+[m
[32m+[m[32m    # ãƒšãƒ¼ã‚¸ãƒ†ã‚­ã‚¹ãƒˆ[m
     text = soup.get_text(" ")[m
 [m
[31m-    def f(p): [m
[31m-        m = re.search(p, text)[m
[31m-        return norm_ws(m.group(1 if m and m.lastindex else 0)) if m else None[m
[32m+[m[32m    def f(pat: str) -> str | None:[m
[32m+[m[32m        m = re.search(pat, text)[m
[32m+[m[32m        if not m:[m
[32m+[m[32m            return None[m
[32m+[m[32m        g = m.group(1 if m.lastindex else 0)[m
[32m+[m[32m        return norm_ws(g)[m
 [m
[31m-    rate = f(r"è£œåŠ©ç‡[\s:ï¼š]*([0-9ï¼-ï¼™]+ ?%?)")[m
[31m-    cap  = f(r"ä¸Šé™[\s:ï¼š]*([0-9ï¼-ï¼™,ï¼Œ]+ ?(?:å††|ä¸‡å††|å„„å††)?)")[m
[31m-    fy   = f(r"(ä»¤å’Œ\s*[0-9ï¼-ï¼™]+å¹´åº¦)")[m
[31m-    cn   = f(r"ç¬¬\s*([0-9ï¼-ï¼™]+)å›")[m
[32m+[m[32m    # å¹´åº¦ï¼ˆå’Œæš¦ or è¥¿æš¦ï¼‰[m
[32m+[m[32m    fiscal_year = f(r"(ä»¤å’Œ\s*[0-9ï¼-ï¼™]+å¹´åº¦|20[0-9]{2}å¹´åº¦)")[m
[32m+[m[32m    call_no     = f(r"ç¬¬\s*([0-9ï¼-ï¼™]+)\s*å›")[m
[32m+[m[32m    rate        = f(r"è£œåŠ©ç‡[\s:ï¼š]*([0-9ï¼-ï¼™]+ ?%?)")[m
[32m+[m[32m    cap         = f(r"ä¸Šé™[\s:ï¼š]*([0-9ï¼-ï¼™,ï¼Œ]+ ?(?:å††|ä¸‡å††|å„„å††)?)")[m
 [m
[32m+[m[32m    # å¯¾è±¡/å¯¾è±¡çµŒè²»ï¼ˆè¦‹å‡ºã—ç›´å¾Œã®è¡Œã‚’ãƒ©ãƒ•ã«æŠœãï¼‰[m
     target, cost_items = None, None[m
[31m-    for lab in ("å¯¾è±¡çµŒè²»","å¯¾è±¡è€…","å¯¾è±¡"):[m
[32m+[m[32m    for lab in ("å¯¾è±¡çµŒè²»", "å¯¾è±¡è€…", "å¯¾è±¡"):[m
         m = re.search(lab + r"[\s:ï¼š]*(.+?)\n", text)[m
         if m:[m
             val = norm_ws(m.group(1))[m
[31m-            if "çµŒè²»" in lab: cost_items = val[m
[31m-            else: target = val[m
[31m-[m
[31m-    return dict([m
[31m-      url=url, title=title or "(ç„¡é¡Œ)", summary=clip(summary, 800),[m
[31m-      rate=rate, cap=cap, target=target, cost_items=cost_items,[m
[31m-      deadline=None, fiscal_year=fy, call_no=cn, scheme_type=None,[m
[31m-      period_from=None, period_to=None[m
[31m-    )[m
\ No newline at end of file[m
[32m+[m[32m            if "çµŒè²»" in lab:[m
[32m+[m[32m                cost_items = val[m
[32m+[m[32m            else:[m
[32m+[m[32m                target = val[m
[32m+[m
[32m+[m[32m    return {[m
[32m+[m[32m        "url": url,[m
[32m+[m[32m        "title": title or "(ç„¡é¡Œ)",[m
[32m+[m[32m        "summary": clip(summary, 800),[m
[32m+[m[32m        "rate": rate,[m
[32m+[m[32m        "cap": cap,[m
[32m+[m[32m        "target": target,[m
[32m+[m[32m        "cost_items": cost_items,[m
[32m+[m[32m        "deadline": None,[m
[32m+[m[32m        "fiscal_year": fiscal_year,[m
[32m+[m[32m        "call_no": call_no,[m
[32m+[m[32m        "scheme_type": None,[m
[32m+[m[32m        "period_from": None,[m
[32m+[m[32m        "period_to": None,[m
[32m+[m[32m    }[m
\ No newline at end of file[m
[1mdiff --git a/lib/http_client.py b/lib/http_client.py[m
[1mindex a15e79e..8ec20bd 100644[m
[1m--- a/lib/http_client.py[m
[1m+++ b/lib/http_client.py[m
[36m@@ -21,15 +21,15 @@[m [mdef head_ok(u: str, ct=3, rt=5) -> bool:[m
         return False[m
 [m
 def conditional_fetch(u: str, etag: str | None, last_mod: str | None):[m
[31m-    """If-None-Match / If-Modified-Since ã‚’ä»˜ã‘ã¦å–å¾—ã€‚304ãªã‚‰ (None, etag, last_mod) ã‚’è¿”ã™ã€‚"""[m
     hdr = dict(HEADERS)[m
     if etag: hdr["If-None-Match"] = etag[m
     if last_mod: hdr["If-Modified-Since"] = last_mod[m
     t0 = time.time()[m
     r = S.get(u, headers=hdr, timeout=(CONNECT, READ))[m
     took_ms = int((time.time() - t0) * 1000)[m
[31m-    r.raise_for_status() if r.status_code != 304 else None[m
[31m-    new_etag = r.headers.get("ETag") or etag[m
[31m-    new_lm   = r.headers.get("Last-Modified") or last_mod[m
[31m-    return (None if r.status_code == 304 else r.text, new_etag, new_lm, r.status_code, took_ms)[m
[32m+[m[32m    ctype = (r.headers.get("Content-Type") or "").split(";")[0].lower()[m
[32m+[m[32m    if r.status_code == 304:[m
[32m+[m[32m        return None, etag, last_mod, ctype, r.status_code, took_ms[m
[32m+[m[32m    r.raise_for_status()[m
[32m+[m[32m    return r.text, r.headers.get("ETag") or etag, r.headers.get("Last-Modified") or last_mod, ctype, r.status_code, took_ms[m
     [m
\ No newline at end of file[m
